{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from skimage import io\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.spatial.distance import hamming, cosine\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# import keras library\n",
    "import keras\n",
    "\n",
    "# import Sequential from the keras models module\n",
    "from keras import Sequential\n",
    "\n",
    "# import Dense, Dropout, Flatten, Conv2D, MaxPooling2D from the keras layers module\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(image_path, image_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, image_size, cv2.INTER_CUBIC)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preprocessing(dataset_path, labels_file_path, image_size, image_paths_pickle):\n",
    "    with open(labels_file_path, \"r\") as f:\n",
    "        classes = f.read().split(\"\\n\")\n",
    "       \n",
    "    images = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "    print(classes)\n",
    "    \n",
    "    for image_name in os.listdir(dataset_path):\n",
    "        try:\n",
    "            image_path = os.path.join(dataset_path, image_name)\n",
    "            images.append(image_loader(image_path, image_size))\n",
    "            image_paths.append(image_path)\n",
    "            \n",
    "            for idx in range(len(classes)):\n",
    "                if classes[idx] in image_name:\n",
    "                    labels.append(idx)\n",
    "                    break\n",
    "                elif len(classes) == idx:\n",
    "                    labels.append(len(classes))\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    with open(image_paths_pickle + \".pickle\", \"wb\") as f:\n",
    "        pickle.dump(image_paths, f)\n",
    "    \n",
    "   # assert len(images) == len(labels)\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"storage/dataset/JPEGImages/\"\n",
    "label_path = \"labels.txt\"\n",
    "batch_size = 128\n",
    "num_classes = 1\n",
    "epochs = 12\n",
    "image_size = (120,72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bus', 'bicycle', 'car', 'motorbike', 'pedestrian', 'trafficlight', 'trafficsignal', '']\n"
     ]
    }
   ],
   "source": [
    "X, y = dataset_preprocessing(dataset_path, label_path, image_size, \"test_image_pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106920, 72, 120, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106920, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model constants\n",
    "num_classes = 8\n",
    "\n",
    "# define model as Sequential\n",
    "model = Sequential()\n",
    "\n",
    "# first convolutional layer with 32 filters\n",
    "model.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=(72, 120, 3)))\n",
    "\n",
    "# add a second 2D convolutional layer with 64 filters\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 70, 118, 8)        224       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 68, 116, 16)       1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 34, 58, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 56, 32)        4640      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 57344)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1835040   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 1,841,336\n",
      "Trainable params: 1,841,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# reduce dimensionality through max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# third convolutional layer with 64 filters\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "# add dropout to prevent over fitting\n",
    "model.add(Dropout(0.25))\n",
    "# necessary flatten step preceeding dense layer\n",
    "model.add(Flatten())\n",
    "# fully connected layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# add additional dropout to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# prediction layers\n",
    "model.add(Dense(num_classes, activation='sigmoid', name='preds'))\n",
    "\n",
    "# show model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80190 samples, validate on 26730 samples\n",
      "Epoch 1/12\n",
      "80190/80190 [==============================] - 286s 4ms/step - loss: 1.5334 - accuracy: 0.5417 - val_loss: 1.4073 - val_accuracy: 0.5484\n",
      "Epoch 2/12\n",
      "80190/80190 [==============================] - 294s 4ms/step - loss: 2.6326 - accuracy: 0.5302 - val_loss: 2.4316 - val_accuracy: 0.5484\n",
      "Epoch 3/12\n",
      "80190/80190 [==============================] - 296s 4ms/step - loss: 3.2056 - accuracy: 0.5275 - val_loss: 2.4316 - val_accuracy: 0.5484\n",
      "Epoch 4/12\n",
      "80190/80190 [==============================] - 297s 4ms/step - loss: 3.5704 - accuracy: 0.5150 - val_loss: 2.4316 - val_accuracy: 0.5484\n",
      "Epoch 5/12\n",
      "80190/80190 [==============================] - 295s 4ms/step - loss: 5.3833 - accuracy: 0.4640 - val_loss: 2.4316 - val_accuracy: 0.5484\n",
      "Epoch 6/12\n",
      "80190/80190 [==============================] - 297s 4ms/step - loss: 5.3192 - accuracy: 0.4696 - val_loss: 2.4316 - val_accuracy: 0.5484\n",
      "Epoch 7/12\n",
      "80190/80190 [==============================] - 297s 4ms/step - loss: 4.9030 - accuracy: 0.4841 - val_loss: 2.4316 - val_accuracy: 0.5484\n",
      "Epoch 8/12\n",
      "80190/80190 [==============================] - 297s 4ms/step - loss: 2.6832 - accuracy: 0.5349 - val_loss: 2.4316 - val_accuracy: 0.5484\n",
      "Epoch 9/12\n",
      "80190/80190 [==============================] - 296s 4ms/step - loss: 2.6930 - accuracy: 0.5343 - val_loss: 2.4316 - val_accuracy: 0.5484\n",
      "Epoch 10/12\n",
      "80190/80190 [==============================] - 297s 4ms/step - loss: 2.6872 - accuracy: 0.5334 - val_loss: 2.4316 - val_accuracy: 0.5484\n",
      "Epoch 11/12\n",
      "80190/80190 [==============================] - 297s 4ms/step - loss: 2.6612 - accuracy: 0.5358 - val_loss: 2.4316 - val_accuracy: 0.5484\n",
      "Epoch 12/12\n",
      "80190/80190 [==============================] - 297s 4ms/step - loss: 2.6731 - accuracy: 0.5348 - val_loss: 2.4316 - val_accuracy: 0.5484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0ed02e6c88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    # set the loss as binary_crossentropy\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    # set the optimizer as stochastic gradient descent\n",
    "    #optimizer= 'adam',\n",
    "    optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "    # set the metric as accuracy\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# mock-train the model using the first ten observations of the train and test sets\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.431586436105051\n",
      "Test accuracy: 0.5483726263046265\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test set\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
